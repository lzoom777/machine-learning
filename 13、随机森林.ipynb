{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging与随机森林"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size =3>Bagging是并行式集成学习方法最典型的代表框架。其核心概念在于自助采样（Bootstrap Sampling），给定包含m个样本的数据集，有放回的随机抽取一个样本放入采样集中，经过m次采样，可得到一个和原始数据集一样大小的采样集。我们可以采样得到T个包含m个样本的采样集，然后基于每个采样集训练出一个基学习器，最后将这些基学习器进行组合。这便是Bagging的主要思想。Bagging与Boosting图示如下："
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](27.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "两个随机性。具体如下：\n",
    "\n",
    "- 假设有M个样本，有放回的随机选择M个样本（每次随机选择一个放回后继续选）。\n",
    "\n",
    "- 假设样本有N个特征，在决策时的每个节点需要分裂时，随机地从这N个特征中选取n个特征，满足n<<N，从这n个特征中选择特征进行节点分裂。\n",
    "\n",
    "- 基于抽样的M个样本n个特征按照节点分裂的方式构建决策树。\n",
    "\n",
    "- 按照1~3步构建大量决策树组成随机森林，然后将每棵树的结果进行综合（分类使用投票法，回归可使用均值法）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree(object):\n",
    "    \"\"\"Super class of RegressionTree and ClassificationTree.\n",
    "    Parameters:\n",
    "    -----------\n",
    "    min_samples_split: int\n",
    "        The minimum number of samples needed to make a split when building a tree.\n",
    "    min_impurity: float\n",
    "        The minimum impurity required to split the tree further.\n",
    "    max_depth: int\n",
    "        The maximum depth of a tree.\n",
    "    loss: function\n",
    "        Loss function that is used for Gradient Boosting models to calculate impurity.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, min_samples_split=2, min_impurity=1e-7,\n",
    "                 max_depth=float(\"inf\"), loss=None):\n",
    "        self.root = None  # Root node in dec. tree\n",
    "        # Minimum n of samples to justify split\n",
    "        self.min_samples_split = min_samples_split\n",
    "        # The minimum impurity to justify split\n",
    "        self.min_impurity = min_impurity\n",
    "        # The maximum depth to grow the tree to\n",
    "        self.max_depth = max_depth\n",
    "        # Function to calculate impurity (classif.=>info gain, regr=>variance reduct.)\n",
    "        # 切割树的方法，gini，方差等\n",
    "        self._impurity_calculation = None\n",
    "        # Function to determine prediction of y at leaf\n",
    "        # 树节点取值的方法，分类树：选取出现最多次数的值，回归树：取所有值的平均值\n",
    "        self._leaf_value_calculation = None\n",
    "        # If y is one-hot encoded (multi-dim) or not (one-dim)\n",
    "        self.one_dim = None\n",
    "        # If Gradient Boost\n",
    "        self.loss = loss\n",
    "\n",
    "    def fit(self, X, y, loss=None):\n",
    "        \"\"\" Build decision tree \"\"\"\n",
    "        self.one_dim = len(np.shape(y)) == 1\n",
    "        self.root = self._build_tree(X, y)\n",
    "        self.loss = None\n",
    "\n",
    "    def _build_tree(self, X, y, current_depth=0):\n",
    "        \"\"\" Recursive method which builds out the decision tree and splits X and respective y\n",
    "        on the feature of X which (based on impurity) best separates the data\"\"\"\n",
    "        largest_impurity = 0\n",
    "        best_criteria = None  # Feature index and threshold\n",
    "        best_sets = None  # Subsets of the data\n",
    "\n",
    "        # Check if expansion of y is needed\n",
    "        if len(np.shape(y)) == 1:\n",
    "            y = np.expand_dims(y, axis=1)\n",
    "\n",
    "        # Add y as last column of X\n",
    "        Xy = np.concatenate((X, y), axis=1)\n",
    "\n",
    "        n_samples, n_features = np.shape(X)\n",
    "\n",
    "        if n_samples >= self.min_samples_split and current_depth <= self.max_depth:\n",
    "            # Calculate the impurity for each feature\n",
    "            for feature_i in range(n_features):\n",
    "                # All values of feature_i\n",
    "                feature_values = np.expand_dims(X[:, feature_i], axis=1)\n",
    "                unique_values = np.unique(feature_values)\n",
    "\n",
    "                # Iterate through all unique values of feature column i and\n",
    "                # calculate the impurity\n",
    "                for threshold in unique_values:\n",
    "                    # Divide X and y depending on if the feature value of X at index feature_i\n",
    "                    # meets the threshold\n",
    "                    Xy1, Xy2 = divide_on_feature(Xy, feature_i, threshold)\n",
    "\n",
    "                    if len(Xy1) > 0 and len(Xy2) > 0:\n",
    "                        # Select the y-values of the two sets\n",
    "                        y1 = Xy1[:, n_features:]\n",
    "                        y2 = Xy2[:, n_features:]\n",
    "\n",
    "                        # Calculate impurity\n",
    "                        impurity = self._impurity_calculation(y, y1, y2)\n",
    "\n",
    "                        # If this threshold resulted in a higher information gain than previously\n",
    "                        # recorded save the threshold value and the feature\n",
    "                        # index\n",
    "                        if impurity > largest_impurity:\n",
    "                            largest_impurity = impurity\n",
    "                            best_criteria = {\"feature_i\": feature_i, \"threshold\": threshold}\n",
    "                            best_sets = {\n",
    "                                \"leftX\": Xy1[:, :n_features],  # X of left subtree\n",
    "                                \"lefty\": Xy1[:, n_features:],  # y of left subtree\n",
    "                                \"rightX\": Xy2[:, :n_features],  # X of right subtree\n",
    "                                \"righty\": Xy2[:, n_features:]  # y of right subtree\n",
    "                            }\n",
    "\n",
    "        if largest_impurity > self.min_impurity:\n",
    "            # Build subtrees for the right and left branches\n",
    "            true_branch = self._build_tree(best_sets[\"leftX\"], best_sets[\"lefty\"], current_depth + 1)\n",
    "            false_branch = self._build_tree(best_sets[\"rightX\"], best_sets[\"righty\"], current_depth + 1)\n",
    "            return DecisionNode(feature_i=best_criteria[\"feature_i\"], threshold=best_criteria[\n",
    "                \"threshold\"], true_branch=true_branch, false_branch=false_branch)\n",
    "\n",
    "        # We're at leaf => determine value\n",
    "        leaf_value = self._leaf_value_calculation(y)\n",
    "        return DecisionNode(value=leaf_value)\n",
    "\n",
    "    def predict_value(self, x, tree=None):\n",
    "        \"\"\" Do a recursive search down the tree and make a prediction of the data sample by the\n",
    "            value of the leaf that we end up at \"\"\"\n",
    "\n",
    "        if tree is None:\n",
    "            tree = self.root\n",
    "\n",
    "        # If we have a value (i.e we're at a leaf) => return value as the prediction\n",
    "        if tree.value is not None:\n",
    "            return tree.value\n",
    "\n",
    "        # Choose the feature that we will test\n",
    "        feature_value = x[tree.feature_i]\n",
    "\n",
    "        # Determine if we will follow left or right branch\n",
    "        branch = tree.false_branch\n",
    "        if isinstance(feature_value, int) or isinstance(feature_value, float):\n",
    "            if feature_value >= tree.threshold:\n",
    "                branch = tree.true_branch\n",
    "        elif feature_value == tree.threshold:\n",
    "            branch = tree.true_branch\n",
    "\n",
    "        # Test subtree\n",
    "        return self.predict_value(x, branch)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\" Classify samples one by one and return the set of labels \"\"\"\n",
    "        y_pred = []\n",
    "        for x in X:\n",
    "            y_pred.append(self.predict_value(x))\n",
    "        return y_pred\n",
    "\n",
    "    def print_tree(self, tree=None, indent=\" \"):\n",
    "        \"\"\" Recursively print the decision tree \"\"\"\n",
    "        if not tree:\n",
    "            tree = self.root\n",
    "\n",
    "        # If we're at leaf => print the label\n",
    "        if tree.value is not None:\n",
    "            print(tree.value)\n",
    "        # Go deeper down the tree\n",
    "        else:\n",
    "            # Print test\n",
    "            print(\"%s:%s? \" % (tree.feature_i, tree.threshold))\n",
    "            # Print the true scenario\n",
    "            print(\"%sT->\" % (indent), end=\"\")\n",
    "            self.print_tree(tree.true_branch, indent + indent)\n",
    "            # Print the false scenario\n",
    "            print(\"%sF->\" % (indent), end=\"\")\n",
    "            self.print_tree(tree.false_branch, indent + indent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationTree(DecisionTree):\n",
    "    def _calculate_information_gain(self, y, y1, y2):\n",
    "        # Calculate information gain\n",
    "        p = len(y1) / len(y)\n",
    "        entropy = calculate_entropy(y)\n",
    "        info_gain = entropy - p * \\\n",
    "                              calculate_entropy(y1) - (1 - p) * \\\n",
    "                                                      calculate_entropy(y2)\n",
    "        # print(\"info_gain\",info_gain)\n",
    "        return info_gain\n",
    "\n",
    "    def _majority_vote(self, y):\n",
    "        most_common = None\n",
    "        max_count = 0\n",
    "        for label in np.unique(y):\n",
    "            # Count number of occurences of samples with label\n",
    "            count = len(y[y == label])\n",
    "            if count > max_count:\n",
    "                most_common = label\n",
    "                max_count = count\n",
    "        # print(\"most_common :\",most_common)\n",
    "        return most_common\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self._impurity_calculation = self._calculate_information_gain\n",
    "        self._leaf_value_calculation = self._majority_vote\n",
    "        super(ClassificationTree, self).fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(700, 20) (700,) (300, 20) (300,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "# 树的棵数\n",
    "n_estimators = 10\n",
    "# 列抽样最大特征数\n",
    "max_features = 15\n",
    "# 生成模拟二分类数据集\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_redundant=0, n_informative=2,\n",
    "                           random_state=1, n_clusters_per_class=1)\n",
    "rng = np.random.RandomState(2)\n",
    "X += 2 * rng.uniform(size=X.shape)\n",
    "# 划分数据集\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 自助抽样选择训练数据子集\n",
    "def bootstrap_sampling(X, y):\n",
    "    X_y = np.concatenate([X, y.reshape(-1,1)], axis=1)\n",
    "    np.random.shuffle(X_y)\n",
    "    n_samples = X.shape[0]\n",
    "    sampling_subsets = []\n",
    "\n",
    "    for _ in range(n_estimators):\n",
    "        # 第一个随机性，行抽样\n",
    "        idx1 = np.random.choice(n_samples, n_samples, replace=True)\n",
    "        bootstrap_Xy = X_y[idx1, :]\n",
    "        bootstrap_X = bootstrap_Xy[:, :-1]\n",
    "        bootstrap_y = bootstrap_Xy[:, -1]\n",
    "        sampling_subsets.append([bootstrap_X, bootstrap_y])\n",
    "    return sampling_subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trees = []\n",
    "# 基于决策树构建森林\n",
    "for _ in range(n_estimators):\n",
    "    tree = ClassificationTree(min_samples_split=2, min_impurity=0,\n",
    "                              max_depth=3)\n",
    "    trees.append(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 随机森林训练\n",
    "def fit(X, y):\n",
    "    # 对森林中每棵树训练一个双随机抽样子集\n",
    "    n_features = X.shape[1]\n",
    "    sub_sets = bootstrap_sampling(X, y)\n",
    "    for i in range(n_estimators):\n",
    "        sub_X, sub_y = sub_sets[i]\n",
    "        # 第二个随机性，列抽样\n",
    "        idx2 = np.random.choice(n_features, max_features, replace=True)\n",
    "        sub_X = sub_X[:, idx2]\n",
    "        trees[i].fit(sub_X, sub_y)\n",
    "        trees[i].feature_indices = idx2\n",
    "        print('The {}th tree is trained done...'.format(i+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForest():\n",
    "    def __init__(self, n_estimators=100, min_samples_split=2, min_gain=0,\n",
    "                 max_depth=float(\"inf\"), max_features=None):\n",
    "        # 树的棵树\n",
    "        self.n_estimators = n_estimators\n",
    "        # 树最小分裂样本数\n",
    "        self.min_samples_split = min_samples_split\n",
    "        # 最小增益\n",
    "        self.min_gain = min_gain\n",
    "        # 树最大深度\n",
    "        self.max_depth = max_depth\n",
    "        # 所使用最大特征数\n",
    "        self.max_features = max_features\n",
    "\n",
    "        self.trees = []\n",
    "        # 基于决策树构建森林\n",
    "        for _ in range(self.n_estimators):\n",
    "            tree = ClassificationTree(min_samples_split=self.min_samples_split, min_impurity=self.min_gain,\n",
    "                                      max_depth=self.max_depth)\n",
    "            self.trees.append(tree)\n",
    "            \n",
    "    # 自助抽样\n",
    "    def bootstrap_sampling(self, X, y):\n",
    "        X_y = np.concatenate([X, y.reshape(-1,1)], axis=1)\n",
    "        np.random.shuffle(X_y)\n",
    "        n_samples = X.shape[0]\n",
    "        sampling_subsets = []\n",
    "\n",
    "        for _ in range(self.n_estimators):\n",
    "            # 第一个随机性，行抽样\n",
    "            idx1 = np.random.choice(n_samples, n_samples, replace=True)\n",
    "            bootstrap_Xy = X_y[idx1, :]\n",
    "            bootstrap_X = bootstrap_Xy[:, :-1]\n",
    "            bootstrap_y = bootstrap_Xy[:, -1]\n",
    "            sampling_subsets.append([bootstrap_X, bootstrap_y])\n",
    "        return sampling_subsets\n",
    "            \n",
    "    # 随机森林训练\n",
    "    def fit(self, X, y):\n",
    "        # 对森林中每棵树训练一个双随机抽样子集\n",
    "        sub_sets = self.bootstrap_sampling(X, y)\n",
    "        n_features = X.shape[1]\n",
    "        # 设置max_feature\n",
    "        if self.max_features == None:\n",
    "            self.max_features = int(np.sqrt(n_features))\n",
    "        \n",
    "        for i in range(self.n_estimators):\n",
    "            # 第二个随机性，列抽样\n",
    "            sub_X, sub_y = sub_sets[i]\n",
    "            idx2 = np.random.choice(n_features, self.max_features, replace=True)\n",
    "            sub_X = sub_X[:, idx2]\n",
    "            self.trees[i].fit(sub_X, sub_y)\n",
    "            # 保存每次列抽样的列索引，方便预测时每棵树调用\n",
    "            self.trees[i].feature_indices = idx2\n",
    "            print('The {}th tree is trained done...'.format(i+1))\n",
    "    \n",
    "    # 随机森林预测\n",
    "    def predict(self, X):\n",
    "        y_preds = []\n",
    "        for i in range(self.n_estimators):\n",
    "            idx = self.trees[i].feature_indices\n",
    "            sub_X = X[:, idx]\n",
    "            y_pred = self.trees[i].predict(sub_X)\n",
    "            y_preds.append(y_pred)\n",
    "            \n",
    "        y_preds = np.array(y_preds).T\n",
    "        res = []\n",
    "        for j in y_preds:\n",
    "            res.append(np.bincount(j.astype('int')).argmax())\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide_on_feature(X, feature_i, threshold):\n",
    "    \"\"\" Divide dataset based on if sample value on feature index is larger than\n",
    "        the given threshold \"\"\"\n",
    "    split_func = None\n",
    "    if isinstance(threshold, int) or isinstance(threshold, float):\n",
    "        split_func = lambda sample: sample[feature_i] >= threshold\n",
    "    else:\n",
    "        split_func = lambda sample: sample[feature_i] == threshold\n",
    "\n",
    "    X_1 = np.array([sample for sample in X if split_func(sample)])\n",
    "    X_2 = np.array([sample for sample in X if not split_func(sample)])\n",
    "\n",
    "    return np.array([X_1, X_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_entropy(y):\n",
    "    \"\"\" Calculate the entropy of label array y \"\"\"\n",
    "    log2 = lambda x: math.log(x) / math.log(2)\n",
    "    unique_labels = np.unique(y)\n",
    "    entropy = 0\n",
    "    for label in unique_labels:\n",
    "        count = len(y[y == label])\n",
    "        p = count / len(y)\n",
    "        entropy += -p * log2(p)\n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionNode():\n",
    "    \"\"\"Class that represents a decision node or leaf in the decision tree\n",
    "    Parameters:\n",
    "    -----------\n",
    "    feature_i: int\n",
    "        Feature index which we want to use as the threshold measure.\n",
    "    threshold: float\n",
    "        The value that we will compare feature values at feature_i against to\n",
    "        determine the prediction.\n",
    "    value: float\n",
    "        The class prediction if classification tree, or float value if regression tree.\n",
    "    true_branch: DecisionNode\n",
    "        Next decision node for samples where features value met the threshold.\n",
    "    false_branch: DecisionNode\n",
    "        Next decision node for samples where features value did not meet the threshold.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, feature_i=None, threshold=None,\n",
    "                 value=None, true_branch=None, false_branch=None):\n",
    "        self.feature_i = feature_i  # Index for the feature that is tested\n",
    "        self.threshold = threshold  # Threshold value for feature\n",
    "        self.value = value  # Value if the node is a leaf in the tree\n",
    "        self.true_branch = true_branch  # 'Left' subtree\n",
    "        self.false_branch = false_branch  # 'Right' subtree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 1th tree is trained done...\n",
      "The 2th tree is trained done...\n",
      "The 3th tree is trained done...\n",
      "The 4th tree is trained done...\n",
      "The 5th tree is trained done...\n",
      "The 6th tree is trained done...\n",
      "The 7th tree is trained done...\n",
      "The 8th tree is trained done...\n",
      "The 9th tree is trained done...\n",
      "The 10th tree is trained done...\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import sys\n",
    "rf = RandomForest(n_estimators=10, max_features=15)\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred = rf.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_score： 0.7566666666666667\n"
     ]
    }
   ],
   "source": [
    "print('accuracy_score：',np.sum(y_test == y_pred, axis=0)/len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
