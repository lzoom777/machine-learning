{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GBDT(Gradient Boosting Decision Tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boosting则是一种集成学习模式，通过将多个单个决策树(弱学习器)进行线性组合构成一个强学习器的过程，Boosting以一个单模型作为作为弱分类器，GBDT中使用CART作为这种弱学习器(基模型)。而融入了梯度下降对Boosting树模型进行优化之后就有了梯度提升树模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 通俗来说，就是通过下一个分类器拟合上一步结果的残差"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](31.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font siza=3>定义代码框架，主要包括树的基本属性和方法。基本属性包括根结点、最小划分样本数、最大深度和是否为叶子结点等等。基本方法包括决策树构建、决策树拟合、决策树预测和打印等方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 提升树"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以决策树为基函数的提升方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 对于分类问题，将adaboost中的基本分类器限制为二类分裂树即可，对于回归问题的提升树算法，对上一步残差拟合"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们先来用一个通俗的说法来理解GBDT。假设某位同学月薪10k，笔者先用一个树模型拟合了6k，发现有4k的损失，然后再用一棵树模型拟合了2k，这样持续拟合下去，拟合值和目标值之间的残差会越来越小，而我们将每一轮迭代，也就是每一棵树的预测值加起来就是模型最终的预测结果。不停的使用单棵决策树组合就是Boosting的过程，使用梯度下降对Boosting树模型进行优化的过程就是Gradient Boosting。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 梯度提升：把损失函数的负梯度作为残差的估计，对于平方损失函数就是，一般的损失函数是近似"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Tree(object):\n",
    "    def __init__(self, min_samples_split=2, min_impurity=1e-7,\n",
    "                 max_depth=float(\"inf\"), loss=None):\n",
    "        self.root = None  # Root node in dec. tree\n",
    "        # Minimum n of samples to justify split\n",
    "        self.min_samples_split = min_samples_split\n",
    "        # The minimum impurity to justify split\n",
    "        self.min_impurity = min_impurity\n",
    "        # The maximum depth to grow the tree to\n",
    "        self.max_depth = max_depth\n",
    "        # Function to calculate impurity (classif.=>info gain, regr=>variance reduct.)\n",
    "        # 切割树的方法，gini，方差等\n",
    "        self._impurity_calculation = None\n",
    "        # Function to determine prediction of y at leaf\n",
    "        # 树节点取值的方法，分类树：选取出现最多次数的值，回归树：取所有值的平均值\n",
    "        self._leaf_value_calculation = None\n",
    "        # If y is one-hot encoded (multi-dim) or not (one-dim)\n",
    "        self.one_dim = None\n",
    "        # If Gradient Boost\n",
    "        self.loss = loss\n",
    "\n",
    "    def fit(self, X, y, loss=None):\n",
    "        \"\"\" Build decision tree \"\"\"\n",
    "        pass\n",
    "\n",
    "    def _build_tree(self, X, y, current_depth=0):\n",
    "        \"\"\" Recursive method which builds out the decision tree and splits X and respective y \"\"\"\n",
    "        pass\n",
    "\n",
    "    def predict_value(self, x, tree=None):\n",
    "        \"\"\" Do a recursive search down the tree and make a prediction of the data sample by the\n",
    "            value of the leaf that we end up at \"\"\"\n",
    "        pass\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\" Classify samples one by one and return the set of labels \"\"\"\n",
    "        pass\n",
    "\n",
    "    def print_tree(self, tree=None, indent=\" \"):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 以回归树为例，基于以上树模型，可定义回归树模型如下：\n",
    "class RegressionTree(Tree):\n",
    "    # 使用方差法进行树分割\n",
    "    def _calculate_variance_reduction(self, y, y1, y2):\n",
    "        var_tot = calculate_variance(y)\n",
    "        var_1 = calculate_variance(y1)\n",
    "        var_2 = calculate_variance(y2)\n",
    "        frac_1 = len(y1) / len(y)\n",
    "        frac_2 = len(y2) / len(y)\n",
    "        # Calculate the variance reduction\n",
    "        variance_reduction = var_tot - (frac_1 * var_1 + frac_2 * var_2)\n",
    "        return sum(variance_reduction)\n",
    "        \n",
    "    # 使用均值法取叶子结点值\n",
    "    def _mean_of_y(self, y):\n",
    "        value = np.mean(y, axis=0)\n",
    "        return value if len(value) > 1 else value[0]\n",
    "        \n",
    "    # 回归树拟合\n",
    "    def fit(self, X, y):\n",
    "        self._impurity_calculation = self._calculate_variance_reduction\n",
    "        self._leaf_value_calculation = self._mean_of_y\n",
    "        super(RegressionTree, self).fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss(object):\n",
    "    def loss(self, y_true, y_pred):\n",
    "        return NotImplementedError()\n",
    "    def gradient(self, y, y_pred):\n",
    "        raise NotImplementedError()\n",
    "    def acc(self, y, y_pred):\n",
    "        return 0\n",
    "        \n",
    "class SquareLoss(Loss):\n",
    "    def __init__(self): pass\n",
    "    def loss(self, y, y_pred):\n",
    "        return 0.5 * np.power((y - y_pred), 2)\n",
    "    def gradient(self, y, y_pred):\n",
    "        return -(y - y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class GBDT(object):\n",
    "    def __init__(self, n_estimators, learning_rate, min_samples_split,\n",
    "                 min_impurity, max_depth, regression):\n",
    "        # 基本参数\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.min_impurity = min_impurity\n",
    "        self.max_depth = max_depth\n",
    "        self.regression = regression\n",
    "        self.loss = SquareLoss()\n",
    "        if not self.regression:\n",
    "            self.loss = SotfMaxLoss()\n",
    "        # 分类问题也可以使用回归树，利用残差去学习概率\n",
    "        self.estimators = []\n",
    "        for i in range(self.n_estimators):\n",
    "            self.estimators.append(RegressionTree(min_samples_split=self.min_samples_split,\n",
    "                                             min_impurity=self.min_impurity,\n",
    "                                             max_depth=self.max_depth))\n",
    "    # 拟合方法\n",
    "    def fit(self, X, y):\n",
    "        # 让第一棵树去拟合模型\n",
    "        print('kaishi')\n",
    "        self.estimators[0].fit(X, y)\n",
    "        print(self.estimators[0])\n",
    "        y_pred = self.estimators[0].predict(X)\n",
    "        print(y)\n",
    "        print(y_pred)\n",
    "        for i in range(1, self.n_estimators):\n",
    "            gradient = self.loss.gradient(y, y_pred) #计算损失\n",
    "            self.estimators[i].fit(X, gradient) #拟合残差\n",
    "            y_pred -= np.multiply(self.learning_rate, self.estimators[i].predict(X))\n",
    "    # 预测方法\n",
    "    def predict(self, X):\n",
    "        y_pred = self.estimators[0].predict(X)\n",
    "        for i in range(1, self.n_estimators):\n",
    "            y_pred -= np.multiply(self.learning_rate, self.estimators[i].predict(X))\n",
    "        if not self.regression:\n",
    "            # Turn into probability distribution\n",
    "            y_pred = np.exp(y_pred) / np.expand_dims(np.sum(np.exp(y_pred), axis=1), axis=1)\n",
    "            # Set label to the value that maximizes probability\n",
    "            y_pred = np.argmax(y_pred, axis=1)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regression tree\n",
    "class GBDTRegressor(GBDT):\n",
    "    def __init__(self, n_estimators=200, learning_rate=0.5, min_samples_split=2,\n",
    "                 min_var_red=1e-7, max_depth=4, debug=False):\n",
    "        super(GBDTRegressor, self).__init__(n_estimators=n_estimators,\n",
    "                                            learning_rate=learning_rate,\n",
    "                                            min_samples_split=min_samples_split,\n",
    "                                            min_impurity=min_var_red,\n",
    "                                            max_depth=max_depth,\n",
    "                                            regression=True)\n",
    "# classification tree\n",
    "class GBDTClassifier(GBDT):\n",
    "    def __init__(self, n_estimators=200, learning_rate=.5, min_samples_split=2,\n",
    "                 min_info_gain=1e-7, max_depth=2, debug=False):\n",
    "        super(GBDTClassifier, self).__init__(n_estimators=n_estimators,\n",
    "                                             learning_rate=learning_rate,\n",
    "                                             min_samples_split=min_samples_split,\n",
    "                                             min_impurity=min_info_gain,\n",
    "                                             max_depth=max_depth,\n",
    "                                             regression=False)\n",
    "    def fit(self, X, y):\n",
    "        y = to_categorical(y)\n",
    "        super(GBDTClassifier, self).fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_data(X, y, seed=None):\n",
    "    \"\"\" Random shuffle of the samples in X and y \"\"\"\n",
    "    if seed:\n",
    "        np.random.seed(seed)\n",
    "    idx = np.arange(X.shape[0])\n",
    "    np.random.shuffle(idx)\n",
    "    return X[idx], y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(X, y, test_size=0.5, shuffle=True, seed=None):\n",
    "    \"\"\" Split the data into train and test sets \"\"\"\n",
    "    if shuffle:\n",
    "        X, y = shuffle_data(X, y, seed)\n",
    "    # Split the training data from test data in the ratio specified in\n",
    "    # test_size\n",
    "    split_i = len(y) - int(len(y) // (1 / test_size))\n",
    "    X_train, X_test = X[:split_i], X[split_i:]\n",
    "    y_train, y_test = y[:split_i], y[split_i:]\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(355, 13) (355,) (151, 13) (151,)\n",
      "kaishi\n",
      "<__main__.RegressionTree object at 0x000002858040EC88>\n",
      "[20.6 20.5 16.1 19.  23.8 15.6 22.2 16.1 15.7 25.  19.4 44.  23.7 20.\n",
      " 22.3 22.6 12.  42.8 13.1 18.2 23.  20.1 18.  13.2 17.4 34.9 22.6 22.8\n",
      " 18.4 19.4  7.  17.8 24.8 17.5 36.2 16.7 19.1  8.4 14.9 23.1 46.   8.7\n",
      " 19.8 45.4 16.6 20.2 17.1 18.5 26.2 19.5 19.3 18.2 34.9 28.7 31.2 24.4\n",
      " 41.7 36.5 19.5 23.  26.4 24.1 19.9 23.9 20.4 23.2 13.3 18.7 30.5 27.5\n",
      " 26.6 15.3 27.1 33.8 23.1 24.6 50.  50.  19.8 50.  13.8 24.6 22.6 17.4\n",
      " 20.6 14.9 33.2 14.5 20.3 11.8 20.3 13.4 29.1 28.1 16.2 17.9 20.7 20.4\n",
      " 14.1 13.9 29.  14.3 16.8 21.6 37.9 43.5 21.2 35.4 18.4 13.1 28.5 17.3\n",
      " 22.2 20.9 32.7 21.7 21.7 20.  37.3 10.8 13.6 20.1  8.4 14.8 13.1 23.4\n",
      " 50.  41.3 14.5 27.1 20.8 18.5 12.5 29.8 22.9 25.  19.8 23.5 22.5  8.8\n",
      " 17.8 46.7 20.1 31.6 23.7 19.7 19.1 22.  12.1 17.1 15.6 15.6 21.2 26.4\n",
      " 43.8 21.4  8.8 24.1 23.  34.7 37.  48.8 28.7 19.6 21.9 24.7 13.8 17.2\n",
      " 38.7 19.2 31.5 15.  28.  18.8 13.4  7.2 25.1 18.3 10.5 39.8 32.9 22.9\n",
      " 13.5 17.6 19.4  8.5 33.1 21.  23.7 20.6 50.  12.7 16.6 20.5 24.8 16.3\n",
      " 17.1 22.8 19.4 22.  21.  23.9 24.  16.1 22.4 15.  22.2 21.9 50.  44.8\n",
      " 17.2 11.  21.5 20.6 24.2 17.8 21.8 22.  22.8 14.1 19.6 50.  30.1 34.9\n",
      " 23.1 20.2  8.3 24.5 13.  14.9 20.1 20.9 12.6 19.4  5.  23.1 32.2 50.\n",
      "  9.5 23.4 23.6 35.2 10.2 11.7 19.4 19.9 11.5 33.  17.4 25.3 12.3 22.\n",
      " 14.1 24.3 13.5 25.  22.9 16.  23.  19.5 22.7 20.8 19.1 23.1 18.8 30.1\n",
      " 19.2 23.2 30.7 22.2 24.4 24.4 23.3 18.6 14.4 21.1 20.6 16.7 50.  20.4\n",
      " 29.6 18.1 12.7 33.2 10.2 50.   7.4 23.2 17.7 31.6 48.5 12.8 10.4 19.5\n",
      " 21.2 17.8 15.2 25.2 23.3 16.5 33.3 10.9 21.4 22.3 34.6 15.1 25.  17.8\n",
      " 22.7 18.9 11.8 31.  22.  16.8 43.1 23.7 22.5 35.4 22.  18.4 24.8 23.3\n",
      " 29.1 36.1 19.6 10.4 21.7 33.4 23.9 24.  18.5 23.9 15.2 50.  10.2 22.2\n",
      " 14.3 13.4 19.3 28.6  9.7 10.5 32.4 13.9 18.9 18.7 29.9 22.4 15.  28.4\n",
      " 29.6 27.9  5.6  8.3 24.4]\n",
      "None\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for -: 'float' and 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-e59b6686210f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGBDTRegressor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;31m# Color map\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-5f3d10e4de02>\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m             \u001b[0mgradient\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#计算损失\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimators\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#拟合残差\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m             \u001b[0my_pred\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimators\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-6640f6e4d18b>\u001b[0m in \u001b[0;36mgradient\u001b[1;34m(self, y, y_pred)\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;36m0.5\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'float' and 'NoneType'"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "boston = datasets.load_boston()\n",
    "X, y = shuffle_data(boston.data, boston.target, seed=13)\n",
    "X = X.astype(np.float32)\n",
    "offset = int(X.shape[0] * 0.9)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
    "\n",
    "model = GBDTRegressor()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "# Color map\n",
    "cmap = plt.get_cmap('viridis')\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print (\"Mean Squared Error:\", mse)\n",
    "\n",
    "# Plot the results\n",
    "m1 = plt.scatter(range(X_test.shape[0]), y_test, color=cmap(0.5), s=10)\n",
    "m2 = plt.scatter(range(X_test.shape[0]), y_pred, color='black', s=10)\n",
    "plt.suptitle(\"Regression Tree\")\n",
    "plt.title(\"MSE: %.2f\" % mse, fontsize=10)\n",
    "plt.xlabel('sample')\n",
    "plt.ylabel('house price')\n",
    "plt.legend((m1, m2), (\"Test data\", \"Prediction\"), loc='lower right')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
